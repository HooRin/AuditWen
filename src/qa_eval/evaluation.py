import json

from lm_eval.base import Task, rf
from bart_score import BARTScorer
import evaluate
import jieba
from rouge_chinese import Rouge
class QAwithString():
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        return {
            "rouge1": (doc["answer"], results[0]),
            "rouge2": (doc["answer"], results[0]),
            "rougeL": (doc["answer"], results[0]),
            "bert_score_f1": (doc["answer"], results[0]),
            "bart_score": (doc["answer"], results[0]),
        }

    def higher_is_better(self):
        return {
            "rouge1": True,
            "rouge2": True,
            "rougeL": True,
            "bert_score_f1": True,
            "bart_score": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def rouge_score(self, items):
        golds, preds = zip(*items)
        rouge = evaluate.load("rouge")
        results = rouge.compute(predictions=preds, references=golds)
        return results

    def rouge1(self, items):
        results = self.rouge_score(items)
        return results["rouge1"]

    def rouge2(self, items):
        results = self.rouge_score(items)
        return results["rouge2"]

    def rougeL(self, items):
        results = self.rouge_score(items)
        return results["rougeL"]

    def is_whitespace_string(s):
        return s.isspace()
    def rougeChinese(self,items):
        hyps, refs = map(list,zip(*[[' '.join(jieba.cut(d[0])), ' '.join(jieba.cut(d[1]))] for d in items]))
        filter_hyps = []
        filter_refs = []
        for i in range(len(hyps)):
            hyp = hyps[i]
            ref = refs[i]
            # if self.is_whitespace_string(hyp) or self.is_whitespace_string(ref):
            #     continue
            if hyp != '' and ref != '':
                filter_hyps.append(hyp)
                filter_refs.append(ref)
        rouge = Rouge()
        scores = rouge.get_scores(filter_hyps, filter_refs, avg=True, ignore_empty=True)
        return scores

    def bert_score(self, items):
        if getattr(self, "_cache_bertscore", None) is None:
            golds, preds = zip(*items)
            bertscore = evaluate.load("/../PIXIU/src/evaluate-metric/bertscore")
            self._cache_bertscore = bertscore.compute(
                predictions=preds,
                references=golds,
                model_type="bert-base-chinese",
            )
            return self._cache_bertscore
        else:
            return self._cache_bertscore

    def bert_score_f1(self, items):
        res = self.bert_score(items)
        return sum(res["f1"]) / len(res["f1"])
        #return res
    def bart_score(self, items):
        golds, preds = zip(*items)
       #bart-base-chinese路径设置
        bart_scorer = BARTScorer(device="cuda", checkpoint="/../bart-base-chinese")
        res = bart_scorer.score(srcs=preds, tgts=golds, batch_size=8)
        return sum(res) / len(res)
        #return res

    def aggregation(self):
        return {
            "rouge1": self.rouge1,
            "rouge2": self.rouge2,
            "rougeL": self.rougeL,
            "bert_score_f1": self.bert_score_f1,
            "bart_score": self.bart_score,
        }



if __name__ == '__main__':
    qa_task = QAwithString()
    #设置数据路径
    json_file_path = ""
    with open(json_file_path, 'r', encoding='utf-8') as file:
        # 使用 json.load() 方法加载 JSON 数据
        data = json.load(file)

    items = []
    for i in data:
        my_tuple =(i["answer"],i["prediction"])
        items.append(my_tuple)

    rouge_chinese = qa_task.rougeChinese(items)
    bert_score_f1 = qa_task.bert_score_f1(items)
    bart_score = qa_task.bart_score(items)

    # Print or use the evaluation results as needed
    print("ROUGE-1:", rouge_chinese["rouge-1"])
    print("ROUGE-2:", rouge_chinese["rouge-2"])
    print("ROUGE-L:", rouge_chinese["rouge-l"])
    print("BERT Score (F1):", bert_score_f1)
    print("BART Score:", bart_score)
