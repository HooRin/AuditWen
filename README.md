# AuditWen
An Open-source Large Language Model for Audit 

## Introduction
The first Chinese open source audit model is based on Qwen-7B-chat and fine-tunes a large amount of supervisory audit related data. The model has a wide range of audit knowledge and intelligent analysis capabilities, aiming to provide comprehensive and effective problem analysis and audit foundation recommendations for the audit field.

## Explain
1. This project is still constantly iterating and updating. Considering the wide coverage of the audit field and the serious shortage of domain dataset resources, we have only released fine-tuning models on the audit laws, regulations, and qualitative question and answer datasets;
2. The data and models provided in this project are for scientific research purposes only and are strictly prohibited from being used for commercial purposes;

## Application Scenarios of LLM in Audit Domain
### Audit issue summary and Laws Recommendation
The primary task of audit is to identify any potential audit issues within a project and determine which laws and regulations can serve as the audit basis. From this perspective, auditors are seeking LLMs to assist in summarizing audit issues based on audit working papers and recommending suitable laws and regulations as both qualitative and punishment basis. 
The primary challenge in the application is that an internal auditor may have a divergent qualitative basis for an audit issue compared to a social auditor based on the case description in the audit working paper. For example, an internal auditor may use items from enterprise internal control manual as qualitative basis without any penalty provision, while a social auditor may refer to items  in Accounting Law and Criminal Law for punishment. To address this challenge, we propose an audit issue schema that summarizes audit issue from case description and aligns them with the clauses of laws and regulations simultaneously。We hope to bridge a gap between the clause of laws and regulations and the audit issue:
<div align="center"><b>Qualitative basis of audit problem and recommended model of punishment basis</b></div>
<p align="center">
    <img src="https://github.com/HooRin/AuditWen/blob/main/pics/pic1.png" alt="Description of the image" width="1200">
</p>



### Audit Relevant Question and Answer
The secondary task of LLM used in audit is to answer question related to audit, such as questions list in Table 1. These questions pertain to defining an audit concept, understanding the specifics of a particular clause of a law, determining the methods for investigating and verifying audit issues, and identifying the necessary data to be collected. These diverse questions prompt us to gather relevant audit documents pertaining to audit cases, audit criteria, audit guidelines, and so on. When assessing the quality of answers generated by LLM, it is crucial to minimize the occurrence of hallucination responses and ensure the retrieval of original text based on existing system documents and other relevant content:
<div align="center"><b>Examples of possible QA proposed by auditor</b></div>

| Query | Answer |
|-------|-------|
| What internal control information does the company need to disclose? (公司需要披露哪些内部控制信息？) | The Company shall fully disclose any internal control information that has a significant impact on investors' investment decisions.(凡对投资者投资决策有重大影响的内部控制信息，公司均应充分披露。) |
| What are the responsibilities of the audit institution under the Internal Audit Regulations? (内部审计条例规定的审计机关的职责有哪些？) | According to Article 23 of Chapter 5 of the Internal Audit Regulations of XX Province, the responsibilities of audit institutions include the following:… (根据XX省内部审计条例第五章第二十三条，审计机关的职责包含以下几项：...) |
| What methods should be adopted in the audit of significant secured loans to identify and conduct detailed investigation? (重大担保借款审计中需要采取哪些方法来识别并展开详细调查？) | Review meeting minutes and materials, documents, contract approval documents, etc., review decision-making process, guarantee scope, scale, etc. (调阅会议纪要及上会材料、文书、合同审批文件等，审核决策流程、担保范围、规模等。) |
| What key information should be collected when conducting a construction project performance audit? (在进行建设项目绩效审计时，应该收集哪些关键资料？) | When conducting a construction project performance audit, key data include: approved project proposals and feasibility study reports,… (在进行建设项目绩效审计时，关键资料包括：已获批准的项目建议书和可行性研究报告，...) |

### Audit assistant  
Further derive requirement of LLM applied in audit domain is LLM can act as an intelligent assistant and help auditor to extract specified phrase from audit document, do accounting relevant numerical calculation, generate an outline for an audit report and further fill content based on the given audit working papers. The possible case questions are list in Table 2. Audit assistant usually need to execute fine-grained NLP task step by step, such as information extraction, multi-documents summarization and document generation. Additionally, audit assistants must achieve collaborative work between humans and machines with the guidance of human-provided knowledge:
<div align="center"><b>The potential tasks that may be assigned to an audit intelligent assistant.</b></div>

| Id | Query |
|-------|-------|
| Q1 | Please extract entity about the audited organization from the following documents. (请从下面文档中抽取出被审计单位信息。) |
| Q2 | Please judge whether Company A is losing money according to the following statement.(请根据下面的报表判断A公司是否亏损？) |
| Q3 | Please write a business leader economic responsibility audit report template. (请撰写出一个企业领导人经济责任审计报告模板。) |
| Q4 | Based on the uploaded audit draft and the generated audit report template, please write the audit process and method of XX leader's accountability audit. (基于上传的审计底稿和生成的审计报告模板，请撰写XX领导人经责审计的审计过程与方法。) |
| Q5 | Based on the uploaded audit ledger list, audit ledgers belonging to the same audit issue are merged into the same document and output. (基于上传的审计台账列表，将属于同一审计问题的审计台账归并到同一文档中并输出。) |

## Modality and Prompts
On the basis of collecting and organizing structured data for various audit tasks, write instruction data.
Taking audit procedures as an example, the table below shows some structured data：
<br>
<div align="center"><b>Raw structured data</b></div>

| 审计类型 | 审计事项 | 审计程序 |
|-------|-------|-------|
|公共工程审计|工程项目内部控制审计|关于工程项目内部控制审计程序，中天恒3C框架研制的工程项目内部控制审计一般程序包括工程项目内部控制有效性调查、初步评价、风险评估、控制测试、评价缺陷、审计评价、形成意见等...|
|公共工程审计|工程项目投资决策审计|工程项目投资决策审计程序：·收集工程项目投资决策相关法律法规。·收集工程项目投资决策相关材料。·调查了解工程项目投资决策情况。·查证核实工程项目投资决策情况（对专业技术文件可委托专业中介机构进行审核，并出具专业审核意见）...|
|公共工程审计|工程项目征地拆迁审计|征地拆迁除要满足一般的审计程序之外，需要特殊考虑的审计程序或工作主要有：...|

Next, construct the corresponding instructions through the following scheme：<br>
<b>
Question：在[审计类型]中，[审计事项]的审计程序是什么？<br>
Answer:[审计程序]<br>
</b>
（更多类型任务的数据参考目录/corpus/Raw structured data下的文件Raw structured data.xlsx）</b>
Based on the above template, the converted instruction data is as follows:
<div align="center"><b>Instruction data set presentation</b></div>

| Query | Answer |
|-------|-------|
|在公共工程审计中，工程项目内部控制审计的审计程序是什么？|关于工程项目内部控制审计程序，中天恒3C框架研制的工程项目内部控制审计一般程序包括工程项目内部控制有效性调查、初步评价、风险评估、控制测试、评价缺陷、审计评价、形成意见等...|
|在公共工程审计中，工程项目投资决策审计的审计程序是什么？|工程项目投资决策审计程序：·收集工程项目投资决策相关法律法规。·收集工程项目投资决策相关材料。·调查了解工程项目投资决策情况。·查证核实工程项目投资决策情况（对专业技术文件可委托专业中介机构进行审核，并出具专业审核意见）...|
|在公共工程审计中，工程项目征地拆迁审计的审计程序是什么？|征地拆迁除要满足一般的审计程序之外，需要特殊考虑的审计程序或工作主要有：...|

The table below summarizes the different tasks, template and examples of an instruction data:
<div align="center"><b>Examples of the instruction data used in LLM tuning dataset</b></div>

| Task name | Template | Examples of an instruction data |
|-------|-------|-------|
| Audit NER | T (1) | "query": 文本: 通过对证券公司和国有企业审计，…。","在上面的文本中，请完成命名实体识别任务，即识别代表审计疑点('auditissue')、机构('ORG')、审计法律法规('auditlbasis')三类实体类型的实体名称，答案应遵循格式\"实体名称, 实体类型\"。"answer": "证券公司, ORG","label": ["O", "O", "O", "B-ORG", "I-ORG", "I-ORG", "I-ORG", "",…] |
| Relation Classification | T (1) | "query":文本：将进口料件表体、报关单表头与加工贸易禁止类商品目录通过商品编号进行关联，筛选出涉及禁止类商品的加工贸易手册。实体对：禁止类商品-加工贸易禁止类商品目录", 根据提供的审计内容和实体对，从['审计问题', '审计事项', '审计依据', '审计方法', '审计机构','审计成果','被审计单位','涉及的行业或领域']中选择能准确表示该实体对关系的选项。请直接回答，如有不确定情况可回答unknown。"answer": 审计依据, "choices": ["unknown", "审计问题", "审计事项", "审计依据", "审计方法", "审计机构", "审计成果", "被审计单位", "涉及的行业或领域"], "gold": 3.  |
| Definition of audit entity | T (2) | "query": 请问什么是营业外收入?, "answer": 该科目核算的是企业发生的与其生产经营无直接关系的各项收入，包括固定资产盘盈、处置固定资产净收益….。 |
| Audit-legal relevant quesiton | T (2) | "query": 如果某公司违反了中华人民共和国证券法第九十条关于征集股东权利的规定，将受到何种法律后果？"answer":根据中华人民共和国证券法第一百九十九条，该公司将被责令改正并给予警告…。 |
| Audit-issue relevant question | T(2) | "query":请问项目单位挤占挪用社会福利基金的表现形式是什么样的？"answer": 项目单位挤占挪用社会福利基金的表现形式：社会福利基金用于投资办企业…。 |
| Other-audit relevant question | T(2) | "query": 请问资源环保审计包括的审计内容是什么?,"answer": 资源环保审计包括的审计内容是土地资源资产,…。 |
| Risk/problem analysis | T (2) | "query": 在国有企业经济责任审计，资产审计可能存在哪些审计风险？"answer": 资产审计可能存在如下风险点：（一）客户管理效率低，没有全面调研客户资质、信用状况并动态跟踪，没有对客户分类，并采取不同的销售政策。…。 || Audit document generation | Audit document generation | Referred in Table 5 |

## Tasks
<div align="center"><b> The details of our evaluation datasets. "Annotation" denotes the construction manner of the instruction data from raw data source. </b></div>

| Level | Task name | Sub-task name | Train | Validation | Test | Annotation |
|-------|-------|-------|-------|-------|-------|-------|
| Sentence level | Audit NER | Audit NER | 4091 | 1022 | 1424 | human annotation |
| Sentence level | Relation Classification | Relation Classification | 817 | 232 | 117 | human annotation |
| Sentence level | Phrase classification | audit entity classification | - | - | 1578 | human annotation |
| Sentence level | Phrase classification | audit-issue phrase classification | 1210 | 344 | 166 | human annotation |
| Sentence level | Phrase classification | legal name classification | 1463 | 418 | 218 | human annotation |
| Paragraph Level | Definition of audit entity | Definition of audit entity | 1756 | 500 | 190 | Extract from raw text |
| Paragraph Level | Audit-legal relevant question | Audit-legal relevant question | 15774 | 112 | 505 | Generated by GPT-4 |
| Paragraph Level | Audit-problem relevant question | audit issue summary | 253 | 71 | 36 | Extract from raw text |
| Paragraph Level | Audit-problem relevant question | audit issue describe | 202 | 56 | 29 | Extract from raw text |
| Paragraph Level |Audit-problem relevant question  | legal recommendation  | 1567 | 445 | 224 | Extract from raw text |
| Paragraph Level | Other-audit relevant question | audit procedures and material  | 671 | 190 | 96 | Extract from raw text |
| Paragraph Level | Other-audit relevant question | audit type and objectives | 609 | 171 | 87 | Extract from raw text |
| Paragraph Level | Other-audit relevant question | other question  | 903 | 257 | 129 | Converted from audit knowledge graph |
| Documents Level | Audit item/risk/problem analysis | Audit item/risk/problem analysis | 544 | 151 | 77 | Extract from raw text |
| Documents Level | Audit case/report generation | Audit case/report generation | 48 | 11 | 6 | Extract from raw text |
| Total |  |  | 29908 | 3980 | 4941 |  |

## The overall performance of different LLMs on audit evaluation benchmark, * denotes 5-shot evaluation for the task.

| Task name | Sub-task name | Metric | Qwen-7B-chat | ChatGLM3-6B | GPT-4 | AuditWen |
|-------|-------|-------|-------|-------|-------|-------|
|NER|NER|entity_F1|0.140*|0.015*|0.108*|0.535*|
|Relation Classification|Relation Classification|accuracy|--/0.085*|0.376/0.342*|0.402/0.624*|0.615/0.188*|
|Relation Classification|Relation Classification|F1|--/0.037*|0.243/0.373*|0.432/0.649*|0.744/0.220*|
|Relation Classification|Relation Classification|missing|0.410/0.00|0.008/0.000|0.000/0.000|0.350/0.274|
|Phrase classification|audit entity classification*|accuracy|0.716/0.763*|0.493/0.540|0.679/0.810*|0.601/0.720*|
|Phrase classification|audit entity classification*|F1|0.710/0.734*|0.583/0.612*|0.697/0.816*|0.612/0.716*|
|Phrase classification|audit entity classification*|missing|0.042/0.00|0.146//0.000|0.023/0.000|0.077/0.000*|
|Phrase classification|audit-issue phrase classification|accuracy|--/0.399*|0.254/0.353*|0.464/0.543*|0.437/0.601*|
|Phrase classification|audit-issue phrase classification|F1|--/0.347*|0.193/0.252*|0.484/0.557*|0.428/0.0.595*|
|Phrase classification|audit-issue phrase classification|missing|0.751/0.000|0.078/0.058|0.000/0.000|0.085/0.037|
|Phrase classification|legal name classification|accuracy|--/0.146*|0.394/0.468*|0.637/0.647*|0.752/0.431*|
|Phrase classification|legal name classification|F1|--/0.075*|0.388/0.428*|0.623/0.639*|0.774/0.405*|
|Phrase classification|legal name classification|missing|0.766/0.165|0.000/0.000|0.004/0.000|0.050/0.037|
|Definition of audit entity|Definition of audit entity|ROUGE-1|0.245|0.22|0.202|0.298|
|Definition of audit entity|Definition of audit entity|ROUGE-2|0.053|0.037|0.037|0.121|
|Definition of audit entity|Definition of audit entity|ROUGE-L|0.178|0.156|0.121|0.237|
|Definition of audit entity|Definition of audit entity|BF1|0.678|0.670|0.662|0.702|
|Definition of audit entity|Definition of audit entity|BART Score|-4.527|-4.535|-4.391|-4.175|
|Audit-legal relevant question|Audit-legal relevant question|BF1|0.696|0.671|0.665|0.723|
|Audit-legal relevant question|Audit-legal relevant question|BART Score|-3.659|-3.356|-3.424|-3.480|
|Audit-issue relevant question|audit issue summary|BF1|0.634|0.644|0.634|0.642|
|Audit-issue relevant question|audit issue summary|BART Score|-4.470|-4.485|-4.524|-4.456|
|Audit-issue relevant question|audit issue describe|BF1	0.696|0.674|0.655|0.792|
|Audit-issue relevant question|audit issue describe|BART Score|-4.048|-3.827|-3.996|-3.044|
|Audit-issue relevant question|legal recommendation|ROUGE-1|0.247|0.268|0.275|0.530|
|Audit-issue relevant question|legal recommendation|ROUGE-2|0.061|0.063|0.083|0.386|
|Audit-issue relevant question|legal recommendation|ROUGE-L|0.150|0.152|0.151|0.442|
|Audit-issue relevant question|legal recommendation|BF1	0.654|0.665|0.677|0.785|
|Audit-issue relevant question|legal recommendation|BART Score|-4.799|-4.192|-3.661|-3.406|
|Other-audit relevant question|audit procedures and material|BF1|0.67|0.682|0.694|0.746|
|Other-audit relevant question|audit procedures and material|BART Score|-5.127|-4.681|-5.166|-4.514|
|Other-audit relevant question|audit items and objectives|BF1|0.723|0.697|0.634|0.907|
|Other-audit relevant question|audit items and objectives|BART Score|-3.794|-3.650|-4.069|-1.587|
|Other-audit relevant question|other question|BF1|0.704|0.663|0.635|0.900|
|Other-audit relevant question|other question|BART Score|-3.284|-3.171|-2.985|-1.202|
|Risk/problem analysis|Risk/problem analysis|BF1|0.67|0.678|0.667|0.84|	
|Risk/problem analysis|Risk/problem analysis|BART Score|-4.854|-3.61|-3.291|-3.031|
|Audit case/report generation|Audit case/report generation|BF1|0.658|0.668|0.670|0.684|
|Audit case/report generation|Audit case/report generation|BART Score|-5.584|-5.003|-4.782|-5.011|


		
		
		
		
	  	
		
		
		
		
		
	 	
		
	
	





## GPT_Q&A
Construct a Q&A session on legal and regulatory content through GPT4.0

## Quick Start
### Directory Structure
----/finetune/ Qwen's finetune Project -- Instruction fine-tuning <br>
----/PIXIU/ PIXIU Project -- NLP Task Inference and Evaluation <br>
----/qa_eval/ Q&A Task Reasoning Reasoning and Evaluation <br>
-----/src/ evaluate related packages <br>
-----/quick_interference.py/ Immediate inference code <br>
-----/interence.py/ task-based inference code <br>
-----/evaluation.py/ evaluation code <br>
--/pics/ Related pictures <br>
---/GPT_Q&A/ <br>
----/code/ GPT4 generates the code for the Q&A <br>
----/data/ partial data <br>
---/corpus/<br>
----/benchmark datatset/ Test set for various tasks <br>
----/result/ Inference results of various task test sets on various pedestals <br>
----/Raw structured data/ Raw Structured data

### Inference Model
To reason with AuditWen, you simply enter a few lines of code, as shown below. Remember to pass the correct model name or path, such as "/model/AuditWen". However, make sure you are using the latest code. (This project also provides quick_interference.py with proxy code located in the directory /AuditWen/src/qa_eval)
```bash
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "/model/AuditWen"
tokenizer = AutoTokenizer.from_pretrained( "/model/AuditWen", trust_remote_code=True)

# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    " /model/AuditWen",
    device_map="auto",
    trust_remote_code=True
).eval()
#You can specify different generation length, top p and other related superparameters
model.generation_config = GenerationConfig.from_pretrained(" /model/AuditWen",, trust_remote_code=True, )  

# 1st dialogue turn
response, history = model.chat(tokenizer, "请问什么是审计范围?", history=None)
print(response)
#审计范围是指审计机构和审计人员在一定的审计目的和审计计划指导下，为完成审计任务所进行的审查所有事项。
#它包括审计对象、审计期间、以及对这些审计对象各个方面所进行的审查程度。

# 2nd dialogue turn
response, history = model.chat(tokenizer, "请问什么是财务报表审计？", history=history)
print(response)
#财务报表审计是指注册会计师对被审计单位编制的财务报表的真实性、合法性、完整性作出审计结论，提出审计意见，形成审计报告的一项审计业务。
#财务报表审计是注册会计师接受委托，按照独立审计准则的要求，对企事业单位的财务报表进行审査，并据此发表审计意见，出具审计报告。所发行的审计报告，为报表使用者做出正确决策提供可靠的参考资料

```


### Fine tuning
Model fine-tuning related content reference https://github.com/QwenLM/Qwen<br>
To prepare the training data, you need to put all the samples into a list and save them to a json file. Each sample is a dictionary, consisting of an id and a list of conversations. Here is a list of simple examples with 1 sample:
```bash
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "请问审计事项'工程设计审计'的审计类型是什么？"
      },
      {
        "from": "assistant",
        "value": "工程设计审计的审计类型是工程项目竣工决算审计。"
      }
    ]
  }
]
```
此外，本项目也提供了包含2000条的训练集位于目录/AuditWen/corpus下，可供使用者参考。<br>
数据准备好后，您可以使用目录/AuditWen/src/finetune/Qwen-main/finetune下提供的shell脚本来运行finetuning。请记住指定数据文件的路径。详细使用可参考Qwen项目的介绍。

### Evaluation-PIXIU
The NLP tasks, namely NER, RE, and NL tasks, are evaluated based on PIXIU. This project has optimized the evaluation method to ensure the accuracy of the evaluation results.
#### Environmental preparation
```bash
cd PIXIU
pip install -r requirements.txt
cd PIXIU/src/financial-evaluation
pip install -e .[multilingual]
```
#### Data path setting
In the __init__.py file under path \PIXIU\src\tasks, set the name of the task and the corresponding method of data processing，Taking "flare_zh_auditner": flare.AuditNER, "as an example, the method for auditing the data loading and processing of the named entity identification task is AuditNER under flare.py.Next, the parameter DATASET_PATH in the AuditNER method is set to the data set path.
#### Evaluation
To evaluate a model hosted on the HuggingFace Hub (for instance, AuditWen), use this command:
```bash
python eval.py \
    --model
    "hf-causal-vllm"
    --model_args
    "pretrained=/model/AuditWen,tokenizer=/model/AuditWen,trust_remote_code=True"
    --tasks
    "flare_zh_auditner"
```

Commercial APIs：
```bash
export OPENAI_API_SECRET_KEY=YOUR_KEY_HERE
python eval.py \
    --model gpt-4 \
    --tasks "flare_zh_auditner"
```

### Evaluation-QA
Model inference is run inference.py under the qa_eval directory, while content evaluation is run evaluation.py under the qa_eval directory
#### Inference
Reasoning data in the form of a visible directory/AuditWen corpus/benchmark datatset q&a type task under json files in the directory.
#### Evaluation
The evaluation method of Q&A tasks is optimized based on the evaluation method of PIXIU Q&A tasks. rouge_chinese is used for evaluation, and the word segmentation "Audit Word segmentation.txt" located in the directory /AuditWen/src/qa_eval/src is loaded for evaluation. The evaluation of bart and bert is also replaced with the Chinese version, and the evaluation effect is somewhat improved compared with PIXIU.<br>
This project provides download links of bart and bert models, users can download to the local, and set the bart path in the corresponding place of the code. Due to the project being deployed Autodl platform, Bert model file location for/root/cache/huggingface/hub/models - Bert - base - Chinese/snapshots/main.<br>
Note: Since the lm_eval and bart_score packages used in the evaluation.py code are all self-contained packages of the project, you need to manually add the package path in the interpreter to ensure the normal operation of the code.

## Model download
AuditWen：https://huggingface.co/HooRin/AuditWen<br>
Qwen-7B-chat：https://huggingface.co/Qwen/Qwen-7B-Chat<br>
ChatGLM3-6B:https://huggingface.co/THUDM/chatglm3-6b<br>
bart-base-chinese:https://huggingface.co/fnlp/bart-base-chinese<br>
bert-base-chinese:https://huggingface.co/google-bert/bert-base-chinese<br>


## Citation
If you use AuditWen in your work, please cite our paper.
@misc{2024AuditWen,
      title={AuditWen：An Open-source Large Language Model for Audit }, 
      author={},
      year={2024},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

## Thanks
This project is based on the secondary development of existing open-source projects. We would like to express our gratitude to the relevant projects and R&D personnel.<br>
1.https://github.com/The-FinAI/PIXIU<br>
2.https://github.com/QwenLM/Qwen

## Contact
E-mail:
